{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd92f29b-02e0-4815-9b57-0f1fa2d7e86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# o código a seguir tem o objetivo de carregar o arquivo contendo os dados após filtragem de linhas e colunas e após feature engeneering\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# carrega arquivo com dados filtrados\n",
    "obj_raras_filtered = pd.read_csv(filepath_or_buffer='raras_retrospectivo_filtered.csv', sep=';', converters={\"hpos\": lambda x: x.strip(\"[]\").split(\", \")})\n",
    "obj_raras_filtered = obj_raras_filtered.drop('Unnamed: 0', axis=1) # isso ou salvar com index=False\n",
    "\n",
    "print(obj_raras_filtered.shape)\n",
    "obj_raras_filtered.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c245cdc5-03a0-4a36-8642-1e256ea79336",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IMPORTAÇÃO DE BIBLIOTECAS E FUNÇÕES\n",
    "# ferramentas básicas para manipular os dados\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "# ferramenta para divisão de conjuntos de teste e treinamento\n",
    "from sklearn.model_selection import train_test_split\n",
    "# ferramentas para composição do fluxo dos dados\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "# ferramentas para processamento dos dados\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn import preprocessing\n",
    "# ferramentas para imputação de dados\n",
    "from sklearn import impute\n",
    "# classificadores\n",
    "from sklearn import neighbors\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn import neural_network\n",
    "from sklearn import linear_model\n",
    "# grid para busca dos parâmetros ótimos\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import GridSearchCV, HalvingGridSearchCV, RandomizedSearchCV, HalvingRandomSearchCV\n",
    "# save the trained model\n",
    "import joblib\n",
    "# configurações de output e warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn import set_config\n",
    "set_config(transform_output=\"pandas\")\n",
    "# manutenção de memória\n",
    "import gc\n",
    "\n",
    "# definido funções custom\n",
    "class NanReplacer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            return X.replace(np.nan, -1)\n",
    "        else:\n",
    "            assert isinstance(X, np.ndarray)\n",
    "            where_are_NaNs = np.isnan(X)\n",
    "            X[where_are_NaNs] = -1\n",
    "            return X\n",
    "\n",
    "class ColunmCleaner(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.loc[:,~X.columns.str.contains('_nan', case=False)]\n",
    "            return X\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)\n",
    "\n",
    "class EmptyHPOSCleaner(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            if 'multilabel_bin__' in X.columns:\n",
    "                X = X.drop(['multilabel_bin__'], axis=1)\n",
    "        return X\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)\n",
    "\n",
    "class MultiHotEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.mlb = None\n",
    "        self.categories_ = self.classes_ = list()\n",
    "    def fit(self, X:pd.DataFrame, y=None):\n",
    "        self.mlb = preprocessing.MultiLabelBinarizer().fit(X)\n",
    "        self.classes_ = self.mlb.classes_\n",
    "        return self\n",
    "    def transform(self, X:pd.DataFrame, y=None):\n",
    "        result = self.mlb.transform(X)\n",
    "        result = pd.DataFrame(\n",
    "            data=result, \n",
    "            columns=self.classes_, \n",
    "            index=X.index\n",
    "        )\n",
    "        return result\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X).transform(X)\n",
    "    def get_feature_names_out(self):\n",
    "        return self.classes_\n",
    "\n",
    "# PREPROCESSAMENTO DOS DADOS, TRANSFORMAÇÃO E ESCALA DE COLUNAS\n",
    "# extrai coluna de classe e aplica label encoder\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "y = obj_raras_filtered.diagnostico\n",
    "obj_raras_filtered_nodiag = obj_raras_filtered.drop(columns=['diagnostico'])\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# separa os dados em conjunto de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(obj_raras_filtered_nodiag, y_encoded, test_size=0.25, stratify=y_encoded)\n",
    "\n",
    "# separar as colunas com valores categóricos\n",
    "categorical_features = [\n",
    "    'tp_diag',\n",
    "    'tp_diag_etiologico',\n",
    "    'momento_diag',\n",
    "    'recorrencia_familiar',\n",
    "    'consang_relatada',\n",
    "    'raca_cor',\n",
    "    'sexo',\n",
    "    'regiao_nascimento',\n",
    "    'regiao_residencia',\n",
    "    'internacao_previa'\n",
    "]\n",
    "\n",
    "# separar as colunas com valores numéricos\n",
    "numeric_features = [\n",
    "    'idade_materna_days',\n",
    "    'idade_paterna_days',\n",
    "    'qtd_internacoes',\n",
    "    'idade_inicio_sintomas_days',\n",
    "    'age_diag_days',\n",
    "    'age_consulta_revisada_days',\n",
    "    'age_1a_consulta_centro_days',\n",
    "    'age_1a_consulta_espec_days'\n",
    "]\n",
    "\n",
    "#fazer onehot com e sem imputer\n",
    "categorical_pipe_imputer = Pipeline(\n",
    "    steps=[\n",
    "        ('one_hot', preprocessing.OneHotEncoder(handle_unknown='ignore', sparse_output=False)),\n",
    "        ('clean_columns', ColunmCleaner()),\n",
    "        ('imputer', impute.SimpleImputer())\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_pipe_no_imputer = Pipeline(\n",
    "    steps=[\n",
    "        ('one_hot', preprocessing.OneHotEncoder(handle_unknown='ignore', sparse_output=False)),\n",
    "        ('clean_columns', ColunmCleaner())\n",
    "    ]\n",
    ")\n",
    "\n",
    "numeric_pipe_replacer = Pipeline(\n",
    "    steps=[\n",
    "        ('scaler', preprocessing.MinMaxScaler()),\n",
    "        ('replacer', NanReplacer())\n",
    "    ]\n",
    ")\n",
    "\n",
    "numeric_pipe_imputer = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', impute.SimpleImputer()),\n",
    "        ('scaler', preprocessing.MinMaxScaler())\n",
    "    ]\n",
    ")\n",
    "\n",
    "# aplica todos os transformadores nas colunas indicadas\n",
    "proprocessing_ct = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('multilabel_bin', MultiHotEncoder(), 'hpos'),\n",
    "        ('categorical_pipe', categorical_pipe_no_imputer, categorical_features),\n",
    "        ('numeric_pipe', numeric_pipe_imputer, numeric_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# pipeline de processamento, limpeza e classificação\n",
    "classification_pipe = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessing_ct', proprocessing_ct),\n",
    "        ('remove_hpo', EmptyHPOSCleaner()),\n",
    "        ('classifier', neighbors.KNeighborsClassifier())\n",
    "    ]\n",
    ")\n",
    "\n",
    "# parâmetros do classificador\n",
    "param_all = [\n",
    "    {\n",
    "      \"preprocessing_ct__numeric_pipe__scaler\": [preprocessing.StandardScaler()],\n",
    "      \"preprocessing_ct__numeric_pipe__imputer__strategy\": [\"mean\"],\n",
    "      \"preprocessing_ct__numeric_pipe__imputer\": [impute.SimpleImputer()],\n",
    "      \"preprocessing_ct__numeric_pipe\": [Pipeline(steps=[\n",
    "          ('imputer', impute.SimpleImputer()), \n",
    "          ('scaler', preprocessing.StandardScaler())\n",
    "        ])],\n",
    "      \"preprocessing_ct__multilabel_bin\": [MultiHotEncoder()],\n",
    "      \"preprocessing_ct__categorical_pipe__imputer__strategy\": [\"mean\"],\n",
    "      \"preprocessing_ct__categorical_pipe__imputer\": [impute.SimpleImputer()],\n",
    "      \"preprocessing_ct__categorical_pipe\": [Pipeline(steps=[('one_hot',\n",
    "        preprocessing.OneHotEncoder(handle_unknown='ignore', sparse_output=False)),\n",
    "        ('clean_columns', ColunmCleaner()),\n",
    "        ('imputer', impute.SimpleImputer())\n",
    "        ])],\n",
    "      \"classifier__weights\": [\"distance\"],\n",
    "      \"classifier__p\": [1],\n",
    "      \"classifier__n_neighbors\": [9],\n",
    "      \"classifier__algorithm\": [\"kd_tree\"],\n",
    "      \"classifier\": [neighbors.KNeighborsClassifier()]\n",
    "    },\n",
    "    {\n",
    "      \"preprocessing_ct__numeric_pipe__scaler\": [preprocessing.MinMaxScaler()],\n",
    "      \"preprocessing_ct__numeric_pipe__imputer\": [impute.SimpleImputer()],\n",
    "      \"preprocessing_ct__numeric_pipe\": [Pipeline(steps=[\n",
    "          ('imputer', impute.SimpleImputer(strategy='most_frequent')),\n",
    "          ('scaler', preprocessing.MinMaxScaler())\n",
    "      ])],\n",
    "      \"preprocessing_ct__multilabel_bin\": [MultiHotEncoder()],\n",
    "      \"preprocessing_ct__categorical_pipe__imputer__weights\": [\"distance\"],\n",
    "      \"preprocessing_ct__categorical_pipe__imputer__n_neighbors\": [3],\n",
    "      \"preprocessing_ct__categorical_pipe__imputer\": [impute.KNNImputer()],\n",
    "      \"preprocessing_ct__categorical_pipe\": [Pipeline(steps=[\n",
    "          ('one_hot', preprocessing.OneHotEncoder(handle_unknown='ignore', sparse_output=False)),\n",
    "          ('clean_columns', ColunmCleaner()),\n",
    "          ('imputer', impute.KNNImputer(n_neighbors=3, weights='distance'))\n",
    "        ])],\n",
    "      \"classifier__solver\": [\"saga\"],\n",
    "      \"classifier__penalty\": [\"l2\"],\n",
    "      \"classifier__l1_ratio\": [0.5],\n",
    "      \"classifier__class_weight\": [None],\n",
    "      \"classifier__C\": [1],\n",
    "      \"classifier\": [linear_model.LogisticRegression()]\n",
    "    },\n",
    "    {\n",
    "      \"preprocessing_ct__numeric_pipe__scaler\": [preprocessing.StandardScaler()],\n",
    "      \"preprocessing_ct__numeric_pipe__imputer__strategy\": [\"mean\"],\n",
    "      \"preprocessing_ct__numeric_pipe__imputer\": [impute.SimpleImputer()],\n",
    "      \"preprocessing_ct__numeric_pipe\": [Pipeline(steps=[\n",
    "          ('imputer', impute.SimpleImputer()),\n",
    "          ('scaler', preprocessing.StandardScaler())\n",
    "      ])],\n",
    "      \"preprocessing_ct__multilabel_bin\": [MultiHotEncoder()],\n",
    "      \"preprocessing_ct__categorical_pipe\": [Pipeline(steps=[\n",
    "          ('one_hot', preprocessing.OneHotEncoder(handle_unknown='ignore', sparse_output=False)),\n",
    "          ('clean_columns', ColunmCleaner())\n",
    "      ])],\n",
    "      \"classifier__solver\": [\"adam\"],\n",
    "      \"classifier__learning_rate\": [\"adaptive\"],\n",
    "      \"classifier__hidden_layer_sizes\": [100],\n",
    "      \"classifier__early_stopping\": [True],\n",
    "      \"classifier__activation\": [\"logistic\"],\n",
    "      \"classifier\": [neural_network.MLPClassifier()]\n",
    "    },\n",
    "    {\n",
    "      \"preprocessing_ct__numeric_pipe__scaler\": [preprocessing.MinMaxScaler()],\n",
    "      \"preprocessing_ct__numeric_pipe__replacer\": [NanReplacer()],\n",
    "      \"preprocessing_ct__numeric_pipe\": [Pipeline(steps=[\n",
    "          ('scaler', preprocessing.MinMaxScaler()),\n",
    "          ('replacer', NanReplacer())])\n",
    "        ],\n",
    "      \"preprocessing_ct__multilabel_bin\": [MultiHotEncoder()],\n",
    "      \"preprocessing_ct__categorical_pipe__imputer__weights\": [\"uniform\"],\n",
    "      \"preprocessing_ct__categorical_pipe__imputer__n_neighbors\": [9],\n",
    "      \"preprocessing_ct__categorical_pipe__imputer\": [impute.KNNImputer(n_neighbors=9)],\n",
    "      \"preprocessing_ct__categorical_pipe\": [Pipeline(steps=[\n",
    "          ('one_hot', preprocessing.OneHotEncoder(handle_unknown='ignore', sparse_output=False)),\n",
    "          ('clean_columns', ColunmCleaner()),\n",
    "          ('imputer', impute.KNNImputer(n_neighbors=9))\n",
    "      ])],\n",
    "      \"classifier__n_estimators\": [200],\n",
    "      \"classifier__max_depth\": [None],\n",
    "      \"classifier__criterion\": [\"gini\"],\n",
    "      \"classifier\": [ensemble.RandomForestClassifier()]\n",
    "    }\n",
    "]\n",
    "\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f16f4e1-5f05-4378-ba24-56bc975fd40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cria grid com os parâmetros do objeto acima e os parametros de cross validation\n",
    "grid = RandomizedSearchCV(\n",
    "    classification_pipe,\n",
    "    #param_grid=param_list,\n",
    "    param_distributions=param_all,\n",
    "    cv=5,\n",
    "    refit=True,\n",
    "    scoring='roc_auc_ovr_weighted',\n",
    "    error_score='raise',\n",
    "    #error_score=np.NINF, # ignora erros e continua grid\n",
    "    n_iter=1000,\n",
    "    verbose=3\n",
    ")\n",
    "\n",
    "from sklearn.utils import estimator_html_repr\n",
    "html_pipe = open('models\\\\RandomizedSearchCV_Pipeline.html', 'w', encoding=\"utf-8\") \n",
    "html_pipe.write(estimator_html_repr(grid))\n",
    "html_pipe.close()\n",
    "\n",
    "# limpa variáveis que não serão mais utilizadas da memória\n",
    "if 'obj_raras_identificacao' in globals():\n",
    "    del obj_raras_identificacao\n",
    "if 'obj_raras_diagnostico' in globals():\n",
    "    del obj_raras_diagnostico\n",
    "if 'obj_raras_merged' in globals():\n",
    "    del obj_raras_merged\n",
    "if 'diagnosticos_mais_comuns' in globals():\n",
    "    del diagnosticos_mais_comuns\n",
    "if 'obj_raras_unused' in globals():\n",
    "    del obj_raras_unused\n",
    "if 'obj_raras_filtered' in globals():\n",
    "    del obj_raras_filtered\n",
    "if 'obj_raras_filtered' in globals():\n",
    "    del obj_raras_filtered\n",
    "if 'obj_raras_filtered_nodiag' in globals():\n",
    "    del obj_raras_filtered_nodiag\n",
    "if 'param_list' in globals():\n",
    "    del param_list\n",
    "gc.collect()\n",
    "\n",
    "# cria modelos utilizando os parâmetros, retém o modelo com melhor desempenho\n",
    "grid.fit(X_train, y_train)\n",
    "print('------- FIT FINISHED -------')\n",
    "\n",
    "# cria arquivo e armazena o melhor modelo\n",
    "joblib.dump(grid.best_estimator_, 'raras_grid_best_estimator_all.sav')\n",
    "\n",
    "# cria arquivo com os melhores parâmetros encontrados na otimização\n",
    "joblib.dump(grid.best_params_, 'raras_grid_best_params_all.pkl', compress = 1)\n",
    "\n",
    "# salva arquivo csv dos resultados\n",
    "grid_results = pd.DataFrame(grid.cv_results_)\n",
    "grid_results.to_csv(path_or_buf='grid_results_all.csv', sep=';')\n",
    "\n",
    "print('------- SAVING FILES ENDED -------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a61ce88-98a2-4175-955e-c88e1ce3edb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification metrics\n",
    "from sklearn import metrics\n",
    "# Plot data\n",
    "import matplotlib.pyplot as plt\n",
    "# pretty print\n",
    "import json\n",
    "# open the trained model\n",
    "import joblib\n",
    "# transform test for ROC curve\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "def print_model_metrics(model, X_test, y_test):\n",
    "    \n",
    "    # testa modelo\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_score = model.predict_proba(X_test)\n",
    "    \n",
    "    # imprimi relatorio de métricas por classe\n",
    "    print(\n",
    "        metrics.classification_report(\n",
    "            label_encoder.inverse_transform(y_test),\n",
    "            label_encoder.inverse_transform(y_pred),\n",
    "            target_names=label_encoder.inverse_transform(model.classes_),\n",
    "            labels=label_encoder.inverse_transform(model.classes_),\n",
    "            digits=3\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # imprime principais métricas\n",
    "    print('Accuracy: %.3f' % metrics.accuracy_score(y_test, y_pred))\n",
    "    print('Balanced Accuracy: %.3f' % metrics.balanced_accuracy_score(y_test, y_pred))\n",
    "    print('Top K=2 Accuracy: %.3f' % metrics.top_k_accuracy_score(y_test, y_score, k=2, labels=model.classes_))\n",
    "    \n",
    "    print('Precision (micro): %.3f' % metrics.precision_score(y_test, y_pred, average='micro'))\n",
    "    print('Precision (macro): %.3f' % metrics.precision_score(y_test, y_pred, average='macro'))\n",
    "    print('Precision (weighted): %.3f' % metrics.precision_score(y_test, y_pred, average='weighted'))\n",
    "    \n",
    "    print('Recall (micro): %.3f' % metrics.recall_score(y_test, y_pred, average='micro'))\n",
    "    print('Recall (macro): %.3f' % metrics.recall_score(y_test, y_pred, average='macro'))\n",
    "    print('Recall (weighted): %.3f' % metrics.recall_score(y_test, y_pred, average='weighted'))\n",
    "    \n",
    "    print('F1 (micro): %.3f' % metrics.f1_score(y_test, y_pred, average='micro'))\n",
    "    print('F1 (macro): %.3f' % metrics.f1_score(y_test, y_pred, average='macro'))\n",
    "    print('F1 (weighted): %.3f' % metrics.f1_score(y_test, y_pred, average='weighted'))\n",
    "    \n",
    "    print('F-beta (micro): %.3f' % metrics.fbeta_score(y_test, y_pred, average='micro', beta=0.5))\n",
    "    print('F-beta (macro): %.3f' % metrics.fbeta_score(y_test, y_pred, average='macro', beta=0.5))\n",
    "    print('F-beta (weighted): %.3f' % metrics.fbeta_score(y_test, y_pred, average='weighted', beta=0.5))\n",
    "    \n",
    "    print('Hamming Loss %.3f' % metrics.hamming_loss(y_test, y_pred))\n",
    "    print('Zero One Loss %.3f' % metrics.zero_one_loss(y_test, y_pred))\n",
    "    print('Log Loss %.3f' % metrics.log_loss(y_test, y_score, labels=model.classes_))\n",
    "    \n",
    "    print('Matthews Correlation Coefficient (MCC): %.3f' % metrics.matthews_corrcoef(y_test, y_pred))\n",
    "    \n",
    "    print('Jaccard similarity coefficient score (micro) %3f' % metrics.jaccard_score(y_test, y_pred, average='micro'))\n",
    "    print('Jaccard similarity coefficient score (macro) %3f' % metrics.jaccard_score(y_test, y_pred, average='macro'))\n",
    "    \n",
    "    print('ROC AUC (ovr micro): %.3f' % metrics.roc_auc_score(y_test, y_score, multi_class='ovr', average='micro', labels=model.classes_))\n",
    "    \n",
    "    print('ROC AUC (ovo macro): %.3f' % metrics.roc_auc_score(y_test, y_score, multi_class='ovo', average='macro', labels=model.classes_))\n",
    "    print('ROC AUC (ovo weighted): %.3f' % metrics.roc_auc_score(y_test, y_score, multi_class='ovo', average='weighted', labels=model.classes_))\n",
    "    \n",
    "    # imprime matriz de confusão\n",
    "    print('Confusion Matrix:')\n",
    "    cm = metrics.confusion_matrix(y_test, y_pred, labels=model.classes_)\n",
    "    disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.inverse_transform(model.classes_))\n",
    "    disp.plot(xticks_rotation='vertical')\n",
    "    \n",
    "    # ROC curve for all casses\n",
    "    label_binarizer = LabelBinarizer().fit(label_encoder.inverse_transform(y_train))\n",
    "    y_onehot_test = label_binarizer.transform(label_encoder.inverse_transform(y_test))\n",
    "    \n",
    "    for class_of_interest in label_binarizer.classes_:\n",
    "        class_id = np.flatnonzero(label_binarizer.classes_ == class_of_interest)[0]\n",
    "    \n",
    "        metrics.RocCurveDisplay.from_predictions(\n",
    "            y_onehot_test[:, class_id],\n",
    "            y_score[:, class_id],\n",
    "            name=f\"{class_of_interest} vs the rest\",\n",
    "            color=\"darkorange\",\n",
    "        )\n",
    "        plt.plot([0, 1], [0, 1], \"k--\", label=\"chance level (AUC = 0.5)\")\n",
    "        plt.axis(\"square\")\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(f\"One-vs-Rest ROC curves:\\n{class_of_interest} vs the rest\")\n",
    "        plt.legend(loc='lower center', bbox_to_anchor=(0.5, -0.3, 0, 0))\n",
    "        plt.show()\n",
    "        \n",
    "        metrics.PrecisionRecallDisplay.from_predictions(\n",
    "            y_onehot_test[:, class_id],\n",
    "            y_score[:, class_id],\n",
    "            name=f\"{class_of_interest} vs the rest\",\n",
    "            color=\"darkblue\",\n",
    "        )\n",
    "        plt.legend(loc='lower center', bbox_to_anchor=(0.5, -0.3, 0, 0))\n",
    "        plt.show()\n",
    "\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f778c6f-6881-44d7-88f2-0b22e003b2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTA DADOS DO PROSPECTIVO SOMENTE\n",
    "\n",
    "# load the model from disk\n",
    "loaded_model = joblib.load('raras_grid_best_estimator_all.sav')\n",
    "\n",
    "# carrega arquivo com dados filtrados do prospectivo\n",
    "obj_raras_som_prosp_filtered = pd.read_csv(filepath_or_buffer='raras_somente_prospectivo_filtered.csv', sep=';', converters={\"hpos\": lambda x: x.strip(\"[]\").split(\", \")})\n",
    "obj_raras_som_prosp_filtered = obj_raras_som_prosp_filtered.drop('Unnamed: 0', axis=1) # isso ou salvar com index=False\n",
    "\n",
    "# marcando como \"outros\" diagnósticos que não estavam no conjunto de treinamento\n",
    "obj_raras_som_prosp_filtered.loc[obj_raras_som_prosp_filtered.diagnostico.isin(label_encoder.classes_) == False, ('diagnostico')] = 'Outros ORPHAs'\n",
    "\n",
    "# extrai coluna de classe e aplica label encoder\n",
    "y_som_prosp = obj_raras_som_prosp_filtered.diagnostico\n",
    "y_som_prosp = label_encoder.transform(y_som_prosp)\n",
    "X_som_prosp = obj_raras_som_prosp_filtered.drop(columns=['diagnostico'])\n",
    "X_som_prosp['internacao_previa'] = X_som_prosp['internacao_previa'].astype(object)\n",
    "\n",
    "print(obj_raras_som_prosp_filtered.diagnostico.value_counts())\n",
    "\n",
    "# resultados utilizando dados do prospectivo\n",
    "print_model_metrics(loaded_model, X_som_prosp, y_som_prosp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6185708-b38d-4072-87fd-0dad3aa935be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
